{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acfa036-1922-43e4-97a1-539c3a2fb1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE, SelectKBest\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV, GroupShuffleSplit, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "from utils import get_group_labels, plot_roc_curve, prob_to_binary, get_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b7c7e-351a-438c-9aa8-704aed6655bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg = pd.read_csv(\"EEG_data.csv\")\n",
    "x = eeg.iloc[:,2:13]\n",
    "normalized_x =(x-x.mean())/x.std()\n",
    "normalized_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb10dd-07f4-4eec-9fdc-2c883a339afd",
   "metadata": {},
   "source": [
    "First we don't want to separate samples from the same subject+video combination to test the models robustness to new types of content. To avoid this I'll implement K-fold with the constraint that all samples are in the same fold. To enforce this, I generate groups subj_id.video_id and use Group K-fold from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92ec7f-b4d0-4245-b6bd-d826542f7e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(normalized_x)\n",
    "y = np.array(eeg[\"user-definedlabeln\"])\n",
    "groups = np.zeros(len(eeg),dtype=np.float32)\n",
    "for i in range(len(eeg)):\n",
    "    num = str(int(eeg.iloc[i,:][\"SubjectID\"])) + \".\" + str(int(eeg.iloc[i,:][\"VideoID\"]))\n",
    "    num = float(num)\n",
    "    groups[i] = num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e5c9f-966a-4997-ab48-d43a6f4dcc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_kfold = GroupKFold(n_splits=5)\n",
    "group_lists = []\n",
    "for i, (train_index, test_index) in enumerate(group_kfold.split(X, y, groups)):\n",
    "    group_lists.append((train_index, test_index))\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n",
    "    print(f\"  Test:  index={test_index}, group={groups[test_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7dca1-e98c-4592-8b0f-a89a93157ece",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b131a6f3-16e6-43c3-80b9-87a9f5c751d2",
   "metadata": {},
   "source": [
    "First, I want to generate a hyperparameter grid to find the best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f88a3-eddc-4df4-8ccb-bcf05be40d59",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c296b-2d77-4a1c-864a-59153f3a7101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {'penalty':[None, 'l2', 'l1'], 'C':[0.1,0.25,0.5, 1], \"random_state\": [0], 'solver':['saga']}\n",
    "logreg = LogisticRegression()\n",
    "clf = GridSearchCV(logreg, parameters, cv=group_lists)\n",
    "clf.fit(X, y)\n",
    "print(\"Score of optimal hyperparameter model: \", clf.best_score_)\n",
    "print(\"Optimal hyperparameter: \", clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de55c76-d16a-4f30-bf83-6777419b2e3f",
   "metadata": {},
   "source": [
    "I tested higher C values but the search favored lower ones. So the optimal set of hyperparameters is {'C': 0.1, 'penalty': 'l2', 'random_state': 0, 'solver': 'saga'} with a score of 0.554. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4caa62e-3952-419e-8e8b-66786f17d20f",
   "metadata": {},
   "source": [
    "### Trying with elasticnet\n",
    "Because elasticnet has other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176e732-6e3f-4d3a-be28-5f4f40e40232",
   "metadata": {},
   "outputs": [],
   "source": [
    "elast_parameters = {'penalty':['elasticnet'], 'C':[0.1,0.25,0.5,1], \"random_state\": [0], 'solver':['saga'], 'l1_ratio':[0,0.25,0.5,0.6,0.75,0.85,1]}\n",
    "logreg = LogisticRegression()\n",
    "clf = GridSearchCV(logreg, elast_parameters, cv=group_lists)\n",
    "clf.fit(X, y)\n",
    "print(\"Score of optimal hyperparameter model: \", clf.best_score_)\n",
    "print(\"Optimal hyperparameter: \", clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ed15a-6670-416b-939f-141197cb97e4",
   "metadata": {},
   "source": [
    "The optimal set is now {'C': 0.1, 'l1_ratio': 0.75, 'penalty': 'elasticnet', 'random_state': 0, 'solver': 'saga'} with a score of 0.554, which is marginally better. It's easiest to use the previous hyperparameter set with only l2 loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ce67f5-c9db-4f02-ad13-c014499b6c3b",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ec7cf-243c-421f-8745-3b61e3992d4c",
   "metadata": {},
   "source": [
    "I'll use Removal and K-Best and test both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de889409-bb53-4fa6-841e-b3809ce5e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_n_features(n):\n",
    "    # Create a logistic regression model\n",
    "    clf = LogisticRegression()\n",
    "    col_names = eeg.iloc[:,2:13].columns\n",
    "    \n",
    "    # Use RFE to select the top 10 features\n",
    "    rfe = RFE(clf, n_features_to_select=n)\n",
    "    rfe.fit(X, y)\n",
    "    print(\"Best features through Removal: \", col_names[rfe.support_])\n",
    "    # K-best\n",
    "    selector = SelectKBest(k=n)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    \n",
    "    # Print the selected features\n",
    "    print(\"Best Features through ANOVA: \", col_names[selector.get_support()])\n",
    "    \n",
    "    optimal_params = {'C': 0.1, 'penalty': 'l2', 'random_state': 0, 'solver': 'saga'}\n",
    "    clf = LogisticRegression(**optimal_params)\n",
    "    \n",
    "    f1_X = X[:,rfe.support_]\n",
    "    rfe_score = np.mean(cross_val_score(clf, f1_X, y, cv=group_lists))\n",
    "    print(\"5-Fold CV with Removal: \", np.mean(rfe_score))\n",
    "    \n",
    "    f2_X = X[:,selector.get_support()]\n",
    "    anova_score = np.mean(cross_val_score(clf, f2_X, y, cv=group_lists))\n",
    "    print(\"5-Fold CV with ANOVA: \", np.mean(anova_score))\n",
    "    return rfe_score, anova_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027a40c-f80a-414f-ad82-c533213b867c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfes, anovas = [], []\n",
    "for i in range(1,12):\n",
    "    print(f\"CHECKING FOR {i} FEATURES\")\n",
    "    r_s, a_s = select_n_features(i)\n",
    "    rfes.append(r_s)\n",
    "    anovas.append(a_s)\n",
    "plt.plot(list(range(1,12)), rfes, label=\"RFE\")\n",
    "plt.plot(list(range(1,12)), anovas, label=\"ANOVA\")\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"Cross Validation Score\")\n",
    "plt.legend()\n",
    "plt.title(\"Score of 5-Fold with feature selection methods\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4671956-93fb-4e09-af7e-80a0f547e0af",
   "metadata": {},
   "source": [
    "We don't want to include too little features for the sake of underfitting or not having robust enough data, so I'll select the 7 features given by anova since it performs the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0346e7d9-7a72-4f9f-b6af-a1bf4a3d403b",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7124fa33-4fa4-4a0b-be4f-cba501da2460",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f461fb-4b4f-47a1-af6b-60214e08363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators':[70,85,100], 'learning_rate':[0.05, 0.1, 0.2], \"min_samples_split\":[2,3,5], \"max_depth\":[2,3,4], \"random_state\": [0], 'min_samples_leaf':[1,2,3]}\n",
    "gbc = GradientBoostingClassifier()\n",
    "clf = GridSearchCV(gbc, parameters, cv=group_lists)\n",
    "clf.fit(X, y)\n",
    "print(\"Score of optimal hyperparameter model: \", clf.best_score_)\n",
    "print(\"Optimal hyperparameter: \", clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34dffef-d6ef-43d9-819a-b80743dc4e1a",
   "metadata": {},
   "source": [
    "Score of optimal hyperparameter model:  0.5585033896985742\n",
    "Optimal hyperparameter:  {'learning_rate': 0.2, 'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 70, 'random_state': 0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c617b02-6813-4fc9-89a9-1952b89310c8",
   "metadata": {},
   "source": [
    "# Evaluation of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a4f680-48b9-44ef-bff7-4b7261af9a18",
   "metadata": {},
   "source": [
    "Train-Test Split. Choosing to use less training data so it generalized better. Given that there's not many subjects or videos it's possible people or videos can be excluded from training leading to worse performance. Note that any split where there are many videos of subject 2 in the test split will lead to horrible performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e90b435-ac29-43c5-b7ac-7fdf8990e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, train_size=.7, random_state=0)\n",
    "indices = []\n",
    "for i, (train_index, test_index) in enumerate(gss.split(X, y, groups)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n",
    "    print(f\"  Test:  index={test_index}, group={groups[test_index]}\")\n",
    "    indices.append((train_index, test_index))\n",
    "train_ind = indices[0][0]\n",
    "test_ind = indices[0][1]\n",
    "print(len(train_ind), len(test_ind))\n",
    "print(\"Unique subj-video combinations in test: \", pd.unique(groups[test_index]))\n",
    "train_X, train_y, train_groups = X[train_ind], y[train_ind], groups[train_ind]\n",
    "test_X, test_y, test_groups = X[test_ind], y[test_ind], groups[test_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6023e8ce-48d8-46c9-afc0-eef0da89c9fb",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c331e5d-a31b-49a0-86dc-dabec7f2332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params = {'C': 0.1, 'penalty': 'l2', 'random_state': 0, 'solver': 'saga'}\n",
    "clf = LogisticRegression(**optimal_params)\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Validation\n",
    "test_time_preds = clf.predict_proba(test_X)[:,1]\n",
    "test_preds, test_labels = get_group_labels(test_time_preds, test_y, test_groups)\n",
    "print(\"Predictions: \", test_preds)\n",
    "print(\"Accuracy of Logistic Regression Model on Test: \", accuracy_score(test_labels, prob_to_binary(test_preds)))\n",
    "\n",
    "# Train\n",
    "train_time_preds = clf.predict_proba(train_X)[:,1]\n",
    "train_preds, train_labels = get_group_labels(train_time_preds, train_y, train_groups)\n",
    "print(\"Accuracy of Logistic Regression Model on Train: \", accuracy_score(train_labels, prob_to_binary(train_preds)))\n",
    "\n",
    "plot_roc_curve(train_preds, train_labels, test_preds, test_labels, \"Logistic Regression\")\n",
    "tnr, tpr, thresholds = get_thresholds(test_preds, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243fff2c-39f0-4c08-8fd3-0ddc97cd3922",
   "metadata": {},
   "source": [
    "So we get an we get a test auc of 0.73 and a test accuracy of 0.7, which is okay. The optimal tpr and tnr is either 0.92 and 0.56 or 0.64 and 0.75 respectively. For the purpose of this experiment we should choose to prioritize True Positive rate. The model doesn't balance both well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2162a9ae-c780-4cb4-8749-34eebeb125bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH FEATURE SELECTION\n",
    "feature_inds = np.array([0,3,4,5,6,7,10])\n",
    "f1_train_X, f1_test_X = train_X[:,feature_inds], test_X[:,feature_inds]\n",
    "\n",
    "optimal_params = {'C': 0.25, 'penalty': 'l2', 'random_state': 0, 'solver': 'saga'}\n",
    "clf = LogisticRegression(**optimal_params)\n",
    "clf.fit(f1_train_X, train_y)\n",
    "\n",
    "# Validation\n",
    "test_time_preds = clf.predict_proba(f1_test_X)[:,1]\n",
    "test_preds, test_labels = get_group_labels(test_time_preds, test_y, test_groups)\n",
    "print(\"Accuracy of Logistic Regression Model on Test: \", accuracy_score(test_labels, prob_to_binary(test_preds)))\n",
    "\n",
    "# Train\n",
    "train_time_preds = clf.predict_proba(f1_train_X)[:,1]\n",
    "train_preds, train_labels = get_group_labels(train_time_preds, train_y, train_groups)\n",
    "print(\"Accuracy of Logistic Regression Model on Train: \", accuracy_score(train_labels, prob_to_binary(train_preds)))\n",
    "\n",
    "plot_roc_curve(train_preds, train_labels, test_preds, test_labels, \"Logistic Regression with Feature Selection\")\n",
    "tnr, tpr, thresholds = get_thresholds(test_preds, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facae322-f64e-4050-be8e-5c5532f383d8",
   "metadata": {},
   "source": [
    "We can see that feature selection leads to worse performance for accuracy and auc when it comes to making classifications at the subject-video level scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e772f-165d-4b35-a254-a07f8a7ac205",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d1532-151f-41c7-86b7-61313a2acf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params = {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100, 'random_state': 0}\n",
    "clf = GradientBoostingClassifier(**optimal_params)\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Validation\n",
    "test_time_preds = clf.predict_proba(test_X)[:,1]\n",
    "test_preds, test_labels = get_group_labels(test_time_preds, test_y, test_groups)\n",
    "print(\"Predictions: \", test_preds)\n",
    "print(\"Accuracy of Logistic Regression Model on Test: \", accuracy_score(test_labels, prob_to_binary(test_preds)))\n",
    "\n",
    "# Train\n",
    "train_time_preds = clf.predict_proba(train_X)[:,1]\n",
    "train_preds, train_labels = get_group_labels(train_time_preds, train_y, train_groups)\n",
    "print(\"Accuracy of Logistic Regression Model on Train: \", accuracy_score(train_labels, prob_to_binary(train_preds)))\n",
    "\n",
    "plot_roc_curve(train_preds, train_labels, test_preds, test_labels, \"Gradient Boosted Trees\")\n",
    "tnr, tpr, thresholds = get_thresholds(test_preds, test_labels)\n",
    "print(tnr[6], tpr[6], thresholds[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed830b12-7c95-44b8-9333-d8aab2428b53",
   "metadata": {},
   "source": [
    "It's interesting that the auc goes up by a lot, with an accuracy of around 0.63 for test data. Clearly the best threshold for balancing both is 0.5477, where both tpr is around 0.857 and tnr is 0.812. This is much better than logistic regression however logistic regression gets a higher accuracy. In this scenario we would still prioritize sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e491d-873e-44e8-80be-a5fa356d76ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stat154)",
   "language": "python",
   "name": "stat154env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
