{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f56e08-2aa3-42b9-8f4b-c1a7adcaa9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import prob_to_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc7150-9554-4862-8c2f-eaf7138630e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg = pd.read_csv(\"EEG_data.csv\")\n",
    "x = eeg.iloc[:,2:13]\n",
    "normalized_x =(x-x.mean())/x.std()\n",
    "normalized_x\n",
    "X = np.array(normalized_x)\n",
    "y = np.array(eeg[\"user-definedlabeln\"])\n",
    "groups = np.zeros(len(eeg),dtype=np.float32)\n",
    "for i in range(len(eeg)):\n",
    "    num = str(int(eeg.iloc[i,:][\"SubjectID\"])) + \".\" + str(int(eeg.iloc[i,:][\"VideoID\"]))\n",
    "    num = float(num)\n",
    "    groups[i] = num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a16e85-ad66-49a8-b8b2-56e5279e7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, train_size=.7, random_state=0)\n",
    "indices = []\n",
    "for i, (train_index, test_index) in enumerate(gss.split(X, y, groups)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n",
    "    print(f\"  Test:  index={test_index}, group={groups[test_index]}\")\n",
    "    indices.append((train_index, test_index))\n",
    "train_ind = indices[0][0]\n",
    "test_ind = indices[0][1]\n",
    "print(len(train_ind), len(test_ind))\n",
    "print(\"Unique subj-video combinations in test: \", pd.unique(groups[test_index]))\n",
    "train_X, train_y, train_groups = X[train_ind], y[train_ind], groups[train_ind]\n",
    "test_X, test_y, test_groups = X[test_ind], y[test_ind], groups[test_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd7b6c-e9eb-4a2e-9396-714aa9c23bb4",
   "metadata": {},
   "source": [
    "# Model and Data Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15426b2c-e49c-4217-b6d3-df36c519f05c",
   "metadata": {},
   "source": [
    "I'll need to make a  need to pad the sequences for the batches to be the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2cfe8-16e4-4a16-83fd-75ec3c2fb202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, y, groups):\n",
    "        # standardize batch sizes\n",
    "        groups = torch.tensor(groups)\n",
    "        subj_vids = []\n",
    "        uq_groups = torch.unique(groups)\n",
    "        labels = torch.ones(uq_groups.size(0))\n",
    "        lengths = torch.ones(uq_groups.size(0))\n",
    "        for i in range(uq_groups.size(0)):\n",
    "            x = uq_groups[i]\n",
    "            ind = (groups == x).nonzero()\n",
    "            lengths[i] = ind.size(0)\n",
    "            subj_vids.append(torch.tensor(X[ind], dtype=torch.float32))\n",
    "            labels[i] = np.mean(y[ind][0])\n",
    "        padded_batches = pad_sequence(subj_vids, batch_first=True, padding_value=-100)\n",
    "        \n",
    "        self.X = padded_batches # n_batches x n_timesteps x 11\n",
    "        self.y = labels\n",
    "        self.group_lengths = lengths\n",
    "        self.len = uq_groups.size(0)\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index], self.group_lengths[index]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb5710-2e0e-49ec-9bb7-4fb9620ca209",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "\n",
    "# Instantiate training and test data\n",
    "train_data = Data(train_X, train_y, train_groups)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = Data(test_X, test_y, test_groups)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=test_data.__len__())\n",
    "\n",
    "for batch, (X, y, len) in enumerate(train_dataloader):\n",
    "    print(f\"Batch: {batch+1}\")\n",
    "    print(f\"X shape: {X.shape}\") # need to deal with the extra dimension during training, printing adjustment below\n",
    "    print(f\"X modified shape: {X.view(X.size(0), X.size(1), X.size(3)).shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    print(f\"true length: {len}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c386e4e-6b24-45b3-ad92-3b1b47596a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG_RNN(nn.Module):\n",
    "    def __init__(self, input_size=11, hidden_size=11, output_size=1, num_layers=1, bias=False, dropout=0):\n",
    "        # 11 features by default, only 1 output since binary\n",
    "        super(EEG_RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, nonlinearity=\"relu\", bias=bias, batch_first=True, dropout=dropout)\n",
    "        self.layer1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, x, lens, hidden_state=None):\n",
    "        final_indices = (lens - 1).type(torch.int)\n",
    "        rnn_output, h_n = self.rnn(x, hidden_state)\n",
    "        rnn_output = rnn_output.data[final_indices,:]\n",
    "        h_1 = self.dropout(rnn_output)\n",
    "        h_2 = self.relu(self.layer1(h_1))\n",
    "        layer_norm2 = nn.LayerNorm([h_2.size(0), h_2.size(1)])\n",
    "        h_2 = layer_norm2(h_2)\n",
    "        h_2 = self.dropout(h_2)\n",
    "        output = self.sigmoid(self.layer2(h_2))\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0c76ad-3765-471d-943e-6d458175bd9e",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8a5ba-9d69-41b1-88ea-6bed596adbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "model = EEG_RNN(dropout=0.75)\n",
    "num_epochs = 100\n",
    "loss_values = []\n",
    "#norms = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCELoss()\n",
    "model.train()\n",
    "best_loss = np.inf\n",
    "optimal_params = model.state_dict()\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y, lens in train_dataloader:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        X = X.view(X.size(0), X.size(1), X.size(3))\n",
    "        X = pack_padded_sequence(X, lens, batch_first=True, enforce_sorted=False) # tell model what to ignore\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        pred = model(X, lens)\n",
    "        loss = loss_fn(pred, y.unsqueeze(-1))\n",
    "        loss_values.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # store best performing model\n",
    "        if loss.item() <= best_loss:\n",
    "            best_loss = loss.item()\n",
    "            optimal_params = model.state_dict()\n",
    "        \n",
    "        \"\"\"\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        norms.append(total_norm)\"\"\"\n",
    "# load best model\n",
    "model.load_state_dict(optimal_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d9865-da33-477b-bb50-98db3f742d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate reduction of loss\n",
    "epoch_index = np.linspace(0, num_epochs, 200)\n",
    "\n",
    "print(np.array(loss_values).shape)\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(epoch_index, np.array(loss_values))\n",
    "plt.title(\"Loss per Train Step\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(epoch_index, np.array(norms))\n",
    "plt.title(\"Norm of Gradient per Train Step\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Norm of Gradient\")\n",
    "plt.show()\"\"\"\n",
    "\n",
    "# evaluate success on test set\n",
    "with torch.no_grad():\n",
    "    for X, y, lens in test_dataloader:\n",
    "        print(X.size(0))\n",
    "        X = X.view(X.size(0), X.size(1), X.size(3))\n",
    "        X = pack_padded_sequence(X, lens, batch_first=True, enforce_sorted=False) # tell model what to ignore\n",
    "        outputs = model(X, lens)\n",
    "        \n",
    "        preds = prob_to_binary(outputs.numpy())\n",
    "        rnn_preds = outputs.numpy()\n",
    "        test_labels = y.numpy()\n",
    "        accuracy = accuracy_score(y.numpy(), preds)\n",
    "        print(\"TEST ACCURACY: \", accuracy)\n",
    "        # Plot ROC\n",
    "        test_auc = roc_auc_score(y.numpy(), outputs.numpy())\n",
    "        te_fpr, te_tpr,thresholds = roc_curve(y.numpy(), outputs.numpy())\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"ROC Curve - Recurrent Network\")\n",
    "        plt.plot(te_fpr, te_tpr, label = f\"Test (AUC = {test_auc})\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "    for X,y, lens in DataLoader(dataset=train_data, batch_size=train_data.__len__()):\n",
    "        X = X.view(X.size(0), X.size(1), X.size(3))\n",
    "        print(X.size(0))\n",
    "        X = pack_padded_sequence(X, lens, batch_first=True, enforce_sorted=False) # tell model what to ignore\n",
    "        outputs = model(X, lens)\n",
    "        \n",
    "        preds = prob_to_binary(outputs.numpy())\n",
    "        accuracy = accuracy_score(y.numpy(), preds)\n",
    "        print(\"TRAIN ACCURACY: \", accuracy)\n",
    "        \n",
    "        # Plot ROC\n",
    "        train_auc = roc_auc_score(y.numpy(), outputs.numpy())\n",
    "        tr_fpr, tr_tpr,thresholds = roc_curve(y.numpy(), outputs.numpy())\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"ROC Curve - Recurrent Network\")\n",
    "        plt.plot(te_fpr, te_tpr, label = f\"Train (AUC = {train_auc})\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dad6ad-20f2-4cf7-b7ad-41b624018746",
   "metadata": {},
   "source": [
    "Explain in paper that it had the tendency to underfit or overfit, so prioritizing perfomance on test data led to a model that didn't do well on training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fedca68-2db5-4996-8bb6-1f04e47a1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = [0.52037767, 0.49901844, 0.56520837, 0.48942119, 0.51469586, 0.63132302,\n",
    " 0.49844834, 0.42362165, 0.40950222, 0.42885294, 0.58069607, 0.6115527,\n",
    " 0.51783915, 0.46300492, 0.52220805, 0.5962914,  0.52307969, 0.57684982,\n",
    " 0.55639789, 0.54565716, 0.62444722, 0.59621355, 0.46035665, 0.49301864,\n",
    " 0.50578362, 0.5414315,  0.55775697, 0.5598228,  0.54579473, 0.53754902]\n",
    "gbf_preds = [0.56720806, 0.53665184, 0.62126978, 0.53470766, 0.55230807, 0.63431831,\n",
    " 0.47839086, 0.42208998, 0.43901364, 0.42522622, 0.56182255, 0.60283442,\n",
    " 0.56690808, 0.45534771, 0.54777996, 0.61411145, 0.48358263, 0.56873362,\n",
    " 0.52046399, 0.58780633, 0.64780111, 0.64631479, 0.47580285, 0.54666121,\n",
    " 0.53211724, 0.55532473, 0.54247439, 0.52227227, 0.60761442, 0.53030283]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37bf08e-8b97-44a7-9f9e-7a3b9cbb3194",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbf_fpr, gbf_tpr, lr_thresholds = roc_curve(test_labels, gbf_preds)\n",
    "gbf_auc = np.round(roc_auc_score(test_labels, gbf_preds),3)\n",
    "lr_fpr, lr_tpr, lr_thresholds = roc_curve(test_labels, lr_preds)\n",
    "lr_auc = np.round(roc_auc_score(test_labels, lr_preds),3)\n",
    "rnn_fpr, rnn_tpr, rnn_thresholds = roc_curve(test_labels, rnn_preds)\n",
    "rnn_auc = np.round(roc_auc_score(test_labels, rnn_preds),3)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC Curve - Model Comparison\")\n",
    "plt.plot(gbf_fpr, gbf_tpr, label = f\"Gradient Boosted Trees (AUC = {gbf_auc})\")\n",
    "plt.plot(lr_fpr, lr_tpr, label = f\"Logistic Regression (AUC = {lr_auc})\")\n",
    "plt.plot(rnn_fpr, rnn_tpr, label = f\"RNN (AUC = {rnn_auc})\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a354f3-2900-4b59-994f-100e7523f25c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
